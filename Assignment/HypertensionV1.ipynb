{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> A Study of Factors related to Hypertension in Adults </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to show some of the essential steps of a workflow for building predictive models. The notebook provides a few examples of each step and it is only a very thin slice of what a complete analysis would consist of. \n",
    "\n",
    "The workflow includes:\n",
    "1. **Problem Definition**:  A clear definition of the problem enables us to identify the appropriate data to gather and technique(s) to use in order to solve the problem. For many problems this many require background reading, discussion with domain experts, and layered problem specification. \n",
    "2. **Data Gathering**: We have to know which data to use, where to gather them, and how to make them useful to solve our problem. In many cases, data from multiple sources can provide deeper insights. \n",
    "3. **Exploratory Data Analysis**: Exploratory data analysis (EDA) is an approach of performing initial investigations on our data. EDA normally has descriptive nature and uses graphical statistics to discover patterns, to identify anomalies, to test hypothesis, and to check assumptions regarding our data. \n",
    "4. **Data Cleaning and Wrangling**: Raw data are generally incomplete, inconsistent, and contain many errors. Thus, we need to prepare the data for further processing. Data wrangling is the process of cleaning, structuring, and enriching raw data into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes, such as analytics.\n",
    "5. **Data Modelling**:  Data modelling involves selecting and optiming the machine learning models that generate the best predictive performance based on the data we have. \n",
    "6. **Prediction**: Once we have developed the best predictive model, we can deploy it to make predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertension is a major public health problem and important area of research due to its high prevalence and being major risk factor for cardiovascular diseases and other complications. To assess the prevalence of hypertension and its associated factors this notebook analyzes the data from the NHANES datasets (https://www.cdc.gov/nchs/nhanes/index.htm)\n",
    "\n",
    "We apply the tools of machine learning to predict the factors that are associated with systolic blood pressure in adults.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0. Data Gathering and Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before moving to the next section, we need to import all packages required to do the analysis by calling the following:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Gathering and Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the datasets by calling the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/\"\n",
    "LOCAL_DATA_PATH = os.path.join(\"datasets\", \"nhanes\") + \"/\"\n",
    "FILE_NAME = \"P_DEMO.XPT\"\n",
    "\n",
    "def fetch_nhanes_data(file_name=FILE_NAME, nhanes_url=DOWNLOAD_ROOT,  nhanes_path=LOCAL_DATA_PATH): \n",
    "    os.makedirs(nhanes_path, exist_ok=True)\n",
    "    xpt_path = os.path.join(nhanes_path, file_name) \n",
    "    url = nhanes_url + file_name\n",
    "    urllib.request.urlretrieve(url, xpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_nhanes_data(\"P_DEMO.XPT\",\"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/\",LOCAL_DATA_PATH)\n",
    "fetch_nhanes_data(\"P_BPXO.XPT\",\"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/\",LOCAL_DATA_PATH)\n",
    "fetch_nhanes_data(\"P_BMX.XPT\",\"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/\",LOCAL_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $LOCAL_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_df = pd.read_sas(LOCAL_DATA_PATH + \"P_DEMO.XPT\")\n",
    "bmx_df = pd.read_sas(LOCAL_DATA_PATH + \"P_BMX.XPT\")\n",
    "bpxo_df = pd.read_sas(LOCAL_DATA_PATH + \"P_BPXO.XPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Exploring Data Structure and Features\n",
    "Before performing data analysis, we often need to know the structure of our data. Therefore, we perform the following:\n",
    "- Viewing a small part of our datasets\n",
    "- Viewing data shape\n",
    "- Describing the features contained in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmx_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpxo_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  More data exploring  (todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only the columns that will be used in the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['SEQN','RIAGENDR','RIDAGEYR','DMDEDUC2']\n",
    "demo_sub_df = demo_df[keep_columns]\n",
    "demo_sub_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = [col for col in bpxo_df if col.startswith('BPXOS') | col.startswith('SEQN')]\n",
    "bpxo_sub_df = bpxo_df[keep_columns]\n",
    "bpxo_sub_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns= ['SEQN','BMXWT','BMXHT','BMXBMI']\n",
    "bmx_sub_df = bmx_df[keep_columns]\n",
    "bmx_sub_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the datatables into a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df = demo_sub_df.merge(bpxo_sub_df, how='inner', on='SEQN')\n",
    "hp_df = hp_df.merge(bmx_sub_df,how=\"inner\", on='SEQN')\n",
    "hp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note the missing values \n",
    "hp_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp_df.hist(bins=50, figsize=(20,15)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hp_sub_df = hp_df[['BMXBMI','BPXOSY', 'RIAGENDR','BMXWT','BMXHT','RIDAGEYR']]\n",
    "corr_matrix = hp_df.corr()\n",
    "corr_matrix[\"BPXOSY1\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_desc = pd.DataFrame({'Description': ['Respondent Sequence Number',\n",
    "                                          'The gender of the passenger',\n",
    "                                          'Age in years at screening',\n",
    "                                          'The Education Level Adults 20+',\n",
    "                                          'Systolic 1st Oscillometric reading',\n",
    "                                          'Systolic 2nd Oscillometric reading',\n",
    "                                          'Systolic 3rd Oscillometric reading',\n",
    "                                          'Weight (Kg)',\n",
    "                                          'Standing Height (cm)',\n",
    "                                          'Body Mass Index (Kg/m**2)'], \n",
    "                          'Values': [hp_df[i].unique() for i in hp_df.columns],\n",
    "                          'Number of unique values': [len(hp_df[i].unique()) for i in hp_df.columns]}, \n",
    "                          index = hp_df.columns)\n",
    "\n",
    "feat_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,10))\n",
    "\n",
    "hp_df['age_groups'] = pd.cut(hp_df['RIDAGEYR'], bins=range(20,90,8))\n",
    "\n",
    "# Creating a bar chart of ticket class (Pclass) vs probability of survival (Survived)\n",
    "ax1 = plt.subplot(221)\n",
    "g1 = sns.barplot(x='age_groups', y='BPXOSY', data=hp_df, color='seagreen')\n",
    "plt.ylabel('Systolic Pressure')\n",
    "plt.xlabel('Age')\n",
    "plt.title('Age and Systolic Pressure', size=13)\n",
    "\n",
    "hp_df = hp_df.drop('age_groups', axis=1)\n",
    "# Creating a bar chart of ticket class (Pclass) and gender (Sex) vs probability of survival (Survived)\n",
    "ax2 = plt.subplot(222)\n",
    "g2 = sns.barplot(x='RIAGENDR', y='BPXOSY', data=hp_df, palette='BuGn_r')\n",
    "plt.ylabel('Systolic Pressure')\n",
    "plt.xlabel('Gender')\n",
    "ax2.set_xticklabels(['Male', 'Female'])\n",
    "plt.title('Gender and Systolic Pressure', size=13)\n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.4, wspace = 0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left barchart shows age and systolic blood pressure are correlations\n",
    "\n",
    "The right barchart shows differences between male and female blood pressures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More graph and EDA needed (todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Data Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We do some of the cleaning and attribute adding now before the split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_add_attributes(hp_df):\n",
    "    if 'BPXOSY' not in hp_df.columns:\n",
    "         hp_df['BPXOSY']= (hp_df['BPXOSY1'] + hp_df['BPXOSY2'] + hp_df['BPXOSY3'])/3 \n",
    "    \n",
    "    return hp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_trim_rows(hp_df):\n",
    "    # Remove all rows that do not have the three systolic values\n",
    "    if ('BPXOSY1' in hp_df.columns) & ('BPXOSY2' in hp_df.columns) & ('BPXOSY3' in hp_df.columns):\n",
    "        hp_df = hp_df[hp_df['BPXOSY1'].notna() & hp_df['BPXOSY2'].notna() & hp_df['BPXOSY3'].notna()]\n",
    "    # We are only interested in adults, so let's drop all individuals with an age less than 20\n",
    "    hp_df = hp_df[hp_df['RIDAGEYR'] >= 20]\n",
    "    return hp_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_drop_columns(hp_df):\n",
    "    # We do not need the seqn now (only needed for the merge)\n",
    "    if 'SEQN' in hp_df.columns:\n",
    "        hp_df = hp_df.drop(\"SEQN\",axis=1)\n",
    "    # the systolic numbers have been averaged\n",
    "    if 'BPXOSY1' in hp_df.columns:\n",
    "        hp_df = hp_df.drop('BPXOSY1',axis=1)\n",
    "    if 'BPXOSY2' in hp_df.columns:\n",
    "        hp_df = hp_df.drop('BPXOSY2',axis=1)\n",
    "    if 'BPXOSY3' in hp_df.columns:\n",
    "        hp_df = hp_df.drop(\"BPXOSY3\",axis=1)\n",
    "    \n",
    "    return hp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_add_trim_drop(hp_df):\n",
    "    hp_df = bp_trim_rows(hp_df)\n",
    "    hp_df = bp_add_attributes(hp_df)\n",
    "    hp_df = bp_drop_columns(hp_df)\n",
    "    \n",
    "    return hp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df = bp_add_trim_drop(hp_df)\n",
    "hp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now split into training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(hp_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have some background information that leads us to believe male and female heart rates are different\n",
    "#  so we make sure that we have even split across train and test\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) \n",
    "for train_index, test_index in split.split(hp_df, hp_df['RIAGENDR']):\n",
    "        strat_train_set = hp_df.iloc[train_index]\n",
    "        strat_test_set = hp_df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_train_X = strat_train_set.drop(\"BPXOSY\", axis=1)\n",
    "bp_train_y = strat_train_set[\"BPXOSY\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_train_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set missing values of numerical data to the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender is a categorical field, so we need to remove to do column calculations\n",
    "\n",
    "bp_num = bp_train_X.drop(\"RIAGENDR\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "#bp_num_tr = num_pipeline.fit_transform(bp_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = list(bp_num)\n",
    "cat_attribs = [\"RIAGENDR\"]\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "        ])\n",
    "bp_prepared = full_pipeline.fit_transform(bp_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bp_prepared is in a numpy array. Sometimes it is useful to have the data in a dataframe, so let's build one \n",
    "# we do not use this dataframe for the rest of the notebook, but you may find it useful\n",
    "column_names = num_attribs.copy()\n",
    "column_names.append('Male')\n",
    "column_names.append('Female')\n",
    "bp_prepared_df = pd.DataFrame(bp_prepared, columns=column_names)\n",
    "bp_prepared_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(bp_prepared, bp_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some_data = X.iloc[:5]\n",
    "some_data = bp_prepared[:5]\n",
    "some_labels = bp_train_y.iloc[:5]\n",
    "print(\"Predictions:\", lin_reg.predict(some_data))\n",
    "print(\"Labels:\", list(some_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "systolic_predictions = lin_reg.predict(bp_prepared)\n",
    "lin_mse = mean_squared_error(bp_train_y, systolic_predictions) \n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation (todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the test set : only done at the end of all modeling (once !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = strat_test_set.drop(\"BPXOSY\", axis=1)\n",
    "y_test = strat_test_set[\"BPXOSY\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_prepared = full_pipeline.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "systolic_predictions = lin_reg.predict(bp_prepared)\n",
    "lin_mse = mean_squared_error(y_test, systolic_predictions) \n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
