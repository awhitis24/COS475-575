{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29ce58a2-2b6a-6869-8535-43ffdf2a040c"
   },
   "source": [
    "This notebook explains how we can go about explore and prepare data for model building.The notebook is structured in the following way \n",
    "\n",
    " - Problem Definition\n",
    " - Data Gathering and Import\n",
    " - Data Wrangling/Cleaning\n",
    " - Exploratory Data Analysis\n",
    " - Data Modeling\n",
    " - Prediction\n",
    "\n",
    " References/Source:\n",
    "    - https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb\n",
    "    - https://github.com/usm-cos422-522/courseMaterials/blob/main/Labs/titanic-workflow.ipynb\n",
    "    - https://www.kaggle.com/viveksrinivasan/eda-ensemble-model-top-10-percentile\n",
    "    - https://www.kaggle.com/miteshyadav/comprehensive-eda-with-xgboost-top-10-percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "783f3ec4-bb24-8414-1055-5dc24748ce68"
   },
   "source": [
    "## Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0754314-700d-e91c-6e39-55c083cf085e"
   },
   "source": [
    "#### Goal\n",
    "\n",
    "To forecast bike rental demand in the Capital Bikeshare program in Washington, D.C. by combining historical usage patterns with weather data in order to forecast bike rental demand. \n",
    "\n",
    "Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.\n",
    "\n",
    "#### **Data Fields**\n",
    "\n",
    "* dteday - date\n",
    "* season -  1 = spring, 2 = summer, 3 = fall, 4 = winter\n",
    "* yr - year\n",
    "* mnth - month\n",
    "* hr - hour\n",
    "* holiday - whether the day is considered a holiday\n",
    "* weekday\n",
    "* workingday - whether the day is neither a weekend nor holiday\n",
    "* weathersit -\n",
    "    * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "* temp - temperature in Celsius\n",
    "* atemp - \"feels like\" temperature in Celsius\n",
    "* humidity - relative humidity\n",
    "* windspeed - wind speed\n",
    "* casual - number of non-registered user rentals initiated\n",
    "* registered - number of registered user rentals initiated\n",
    "* cnt - number of total rentals (Dependent Variable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Gathering and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "548437f4-7dcb-60a4-0a79-74d9594b071b"
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# libraries for reading url based files\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "# libraries for recoding fields and pipeline construction\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# libraries for model building\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec94edf8-890a-0ba2-decc-914582ac316d"
   },
   "source": [
    "### **Read In The Dataset from the UCI data repository**\n",
    "\n",
    "The data file is located at https://archive.ics.uci.edu/ml/machine-learning-databases/00275/  We will have to download the zip file and then extract the hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DOWNLOAD_ROOT = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/\"\n",
    "LOCAL_DATA_PATH = os.path.join(\"datasets\", \"bikeshare\") + \"/\"\n",
    "FILE_NAME = \"Bike-Sharing-Dataset.zip\"\n",
    "\n",
    "def fetch_bikeshare_data(file_name = FILE_NAME, bikeshare_url=DOWNLOAD_ROOT,  bikeshare_path=LOCAL_DATA_PATH): \n",
    "    os.makedirs(bikeshare_path, exist_ok=True)\n",
    "    xpt_path = os.path.join(bikeshare_path, file_name) \n",
    "    url = bikeshare_url + file_name\n",
    "    urllib.request.urlretrieve(url, xpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_bikeshare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o datasets/bikeshare/Bike-Sharing-Dataset -d ./datasets/bikeshare\n",
    "!ls datasets/bikeshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/bikeshare/hour.csv',parse_dates=['dteday'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4eab050d-65c1-7c33-5414-c408a553950d"
   },
   "source": [
    "### Exploring Data Structure and Features\n",
    "\n",
    "\n",
    "As a first step lets do three simple steps on the dataset\n",
    "\n",
    " - Size of the dataset\n",
    " - Get a glimpse of data by printing few rows of it.\n",
    " - What type of variables contribute our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "579ef8b7-02ad-7087-f27e-cc817f58f90a"
   },
   "source": [
    "#### **Shape Of The Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4b5eee60-635b-e053-97cd-62f3e9e8acac"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d390d70a-24ba-1e0a-33af-f4bf58862f93"
   },
   "source": [
    "#### **Sample Of First Few Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "664d34ff-4580-aeb1-18ea-6dca2b5ae078"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c553d3a-0363-91e3-49b4-1a74a7b464c9"
   },
   "source": [
    "#### **Variables Data Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ee04436-80f8-ba7c-242e-ec78a9df5fdc"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d3dfb2ef-0038-3dfd-928d-890eeba0c09a"
   },
   "source": [
    " #### Do we have missing values ?\n",
    "  find out whether we have any missing values in our data. Luckily we dont have any missing value in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'Number of Missing Values (Training)': df.isna().sum(),\n",
    "              '% of Missing Values (Training)': (df.isna().sum()/df.shape[0] * 100).round(2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "44d05ac3-327f-26ce-cc4a-f2d1e077a9a0"
   },
   "source": [
    "### Visualize Distribution Of Data\n",
    "As it is visible from the below figures that \"count\" variable is skewed towards right. It is desirable to have Normal distribution as most of the machine learning techniques require dependent variable to be Normal. One possible solution is to take log transformation on \"count\" variable after removing outlier data points. After the transformation the data looks lot better but still not ideally following normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram for count; this is our dependent variable ... let's look close\n",
    "sns.set_style('darkgrid')\n",
    "sns.histplot(df['cnt'], bins = 100, color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot for count\n",
    "# The whiskers extend from the box by 1.5x the inter-quartile range (IQR)\n",
    "import matplotlib.pyplot as plt\n",
    "sns.boxplot(x = 'cnt', data = df, color = 'mediumpurple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three charts above can tell us a lot about our target variable.\n",
    "\n",
    "Our target variable, count is not normally distributed.\n",
    "There are multiple outliers in the variable. We could get rid of outside the 1.5x IQR of 3 standard deviations. We choose the later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and test dataframes\n",
    "\n",
    "Strategy : use the first 24 days of the month as training data and the remaining days are test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cutoff_day = 24\n",
    "train_df = df[df.dteday.dt.day <=cutoff_day]\n",
    "test_df = df[df.dteday.dt.day>cutoff_day]\n",
    "print(\"training rows\", train_df.shape[0])\n",
    "print(\"test rows\", test_df.shape[0])\n",
    "print(\"training ratio\", train_df.shape[0]/df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "687cad46-2854-3af7-bb10-7c0e96153adf"
   },
   "source": [
    "#### **Lets Remove Outliers In The Count Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = train_df[np.abs(train_df[\"cnt\"]-train_df[\"cnt\"].mean())>(3*train_df[\"cnt\"].std())]\n",
    "print((len(outliers)/len(train_df))*100)                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outliers.shape)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data without the outliers in count\n",
    "train_df = train_df[~train_df.isin(outliers)].dropna()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "44d05ac3-327f-26ce-cc4a-f2d1e077a9a0"
   },
   "source": [
    "#### Visualizing Distribution Of Count Data after removing outliers\n",
    "As it is visible from the below figures that \"count\" variable is skewed towards right. It is desirable to have Normal distribution as most of the machine learning techniques require dependent variable to be Normal. One possible solution is to take log transformation on \"count\" variable after removing outlier data points. After the transformation the data looks lot better but still not ideally following normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b0caaf4-4215-e3bc-d6b4-7914696c668f"
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(ncols=2,nrows=1)\n",
    "fig.set_size_inches(6, 5)\n",
    "sns.histplot(train_df[\"cnt\"],ax=axes[0])\n",
    "sns.histplot(np.log(train_df[\"cnt\"]),ax=axes[1])\n",
    "axes[0].set(xlabel='number of rentals', ylabel='Count',title=\"cnt histogram\")\n",
    "axes[1].set(xlabel='log number of rentals',title=\"log cnt histogram\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "One common way to understand how a dependent variable is influenced by features (numerical) is to build a correlation matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of variables we are interested in \n",
    "\n",
    "corr = train_df[['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n",
    "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed','cnt']].corr()\n",
    "corr['cnt'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - temp and humidity features have positive and negative correlation\n",
    "   with count respectively.Although the correlation between them are not\n",
    "   very prominent still the count variable has little dependency on\n",
    "   \"temp\" and \"humidity\".\n",
    " - windspeed is not really useful numerical feature and it is visible from it correlation value with \"count\"\n",
    " - \"atemp\" is variable is not taken into since \"atemp\" and \"temp\" has got strong correlation with each other. During model building any one of the variable has to be dropped since they will exhibit multicollinearity in the data.\n",
    " - \"Casual\" and \"Registered\" are also not taken into account since they are leakage variables in nature and need to dropped during model building.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d15f1bf9-f695-c69a-a673-db356620bcea"
   },
   "source": [
    "#### Visualizing Count Vs (Month,Season,Hour,Weekday,Usertype)\n",
    "\n",
    " - It is quiet obvious that people tend to rent bike during summer\n",
    "   season since it is really conducive to ride bike at that\n",
    "   season.Therefore June, July and August has got relatively higher\n",
    "   demand for bicycle.\n",
    " - On weekdays more people tend to rent bicycle around 7AM-8AM and 5PM-6PM. As we mentioned earlier this can be attributed to regular school and office commuters.\n",
    " - Above pattern is not observed on \"Saturday\" and \"Sunday\".More people tend to rent bicycle between 10AM and 4PM.\n",
    " - The peak user count around 7AM-8AM and 5PM-6PM is purely contributed by registered user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1c8b97d1-8022-a64a-8d7f-60254955b47e"
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3,ax4)= plt.subplots(nrows=4)\n",
    "fig.set_size_inches(12,20)\n",
    "hueOrder = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "\n",
    "monthAggregated = pd.DataFrame(train_df.groupby(\"mnth\")[\"cnt\"].mean()).reset_index()\n",
    "monthSorted = monthAggregated.sort_values(by=\"cnt\",ascending=False)\n",
    "sns.barplot(data=monthSorted,x=\"mnth\",y=\"cnt\",ax=ax1)\n",
    "ax1.set(xlabel='Month', ylabel='Average Count',title=\"Average Count By Month\")\n",
    "\n",
    "ax2.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Season\",label='big')\n",
    "\n",
    "hourAggregated = pd.DataFrame(train_df.groupby([\"hr\",\"weekday\"],sort=True)[\"cnt\"].mean()).reset_index()\n",
    "sns.pointplot(x=hourAggregated[\"hr\"], y=hourAggregated[\"cnt\"],hue=hourAggregated[\"weekday\"], data=hourAggregated, join=True,ax=ax3)\n",
    "ax3.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Weekdays\",label='big')\n",
    "\n",
    "hourTransformed = pd.melt(train_df[[\"hr\",\"casual\",\"registered\"]], id_vars=['hr'], value_vars=['casual', 'registered'])\n",
    "hourAggregated = pd.DataFrame(hourTransformed.groupby([\"hr\",\"variable\"],sort=True)[\"value\"].mean()).reset_index()\n",
    "sns.pointplot(x=hourAggregated[\"hr\"], y=hourAggregated[\"value\"],hue=hourAggregated[\"variable\"],hue_order=[\"casual\",\"registered\"], data=hourAggregated, join=True,ax=ax4)\n",
    "ax4.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across User Type\",label='big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eaf70e2b-13e7-1a57-511f-63b4f5e7ecdb"
   },
   "source": [
    "**So we have visualized the data to a greater extent.So lets go and  build some models and see how close we can predict the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Drop, recode, and normalize columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "105c47bc-b67e-aa6c-2f11-1e7da597dc6a"
   },
   "outputs": [],
   "source": [
    "#categoricalFeatures = [\"season\",\"holiday\",\"workingday\",\"weathersit\",\"weekday\",\"mnth\",\"yr\",\"hr\"]\n",
    "#numericalFeatures = [\"hum\",\"windspeed\",\"atemp\"]\n",
    "#dropFeatures = ['instant','casual',\"dteday\",\"registered\",\"temp\"]\n",
    "\n",
    "categoricalFeatures = [\"weathersit\",\"holiday\",\"season\",\"workingday\",\"weekday\",\"mnth\",\"yr\",\"hr\"]\n",
    "numericalFeatures = [\"hum\",\"windspeed\",\"atemp\"]\n",
    "dropFeatures = ['instant','casual',\"dteday\",\"registered\",\"temp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = train_df.drop(dropFeatures, axis=1)\n",
    "df_num = df_sub[numericalFeatures]\n",
    "df_cat = df_sub[categoricalFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_y = train_df['cnt']\n",
    "bike_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numericalFeatures),\n",
    "        (\"cat\", OneHotEncoder(), categoricalFeatures),\n",
    "        ])\n",
    "bike_X = full_pipeline.fit_transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bike creates a sparse matrix, lets look at the first rows\n",
    "bike_X.todense()[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "53111418-c6cc-b02e-2395-a04e45b1298a"
   },
   "source": [
    "### **Linear Regression Model** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logistic regression model\n",
    "lModel = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "lModel.fit(X = bike_X,y = bike_y)\n",
    "\n",
    "# Make predictions\n",
    "count_preds = lModel.predict(X=bike_X)\n",
    "lin_mse = mean_squared_error(bike_y, count_preds)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse\n",
    "print (\"RMSLE Value For Linear Regression: \",lin_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Evaluation Using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(lModel, bike_X, bike_y,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "rmse_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6487f456-f1dc-680b-51fd-9ec8b45d1bac"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
